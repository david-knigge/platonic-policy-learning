# Platonic Policy Learning — DiT Diffusion Pipeline

This repository hosts a stripped-down diffusion policy training pipeline that reuses the DiT (Diffusion Transformer) backbone from `in_context_imitation_learning`. The focus is purely on training: no evaluation or inference utilities are included.

## Environment

First, we must make sure CoppeliaSim is installed to enable interaction with rlbench environments.

```bash
# set env variables
export COPPELIASIM_ROOT=${HOME}/CoppeliaSim
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT
export QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT

wget https://downloads.coppeliarobotics.com/V4_1_0/CoppeliaSim_Edu_V4_1_0_Ubuntu20_04.tar.xz
mkdir -p $COPPELIASIM_ROOT && tar -xf CoppeliaSim_Edu_V4_1_0_Ubuntu20_04.tar.xz -C $COPPELIASIM_ROOT --strip-components 1
rm -rf CoppeliaSim_Edu_V4_1_0_Ubuntu20_04.tar.xz
```

Next let's create the env.

```bash
mamba create -n ppl python=3.10 -y
mamba activate ppl

# Add env vars
conda env config vars set COPPELIASIM_ROOT=${HOME}/CoppeliaSim LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT

# Install relevant packages
mamba install -n ppl -y numpy==1.26.4 scipy==1.11.4 pillow h5py pyquaternion natsort -c conda-forge
pip install torch==2.2.0 diffusers==0.27.0 huggingface_hub==0.23.0 gymnasium
pip install git+ssh://git@github.com/david-knigge/rlbench_data_gen.git
```

The direct Git install deliberately skips heavy simulator dependencies while exposing `tools.datasets.imitation_learning`. Ensure your SSH key has access to `git@github.com:david-knigge/rlbench_data_gen.git`.

## Dataset Expectations

`train.py` consumes RLBench caches generated by the upstream preprocessing script. A typical invocation looks like:

```bash
PYTHONPATH=. python tools/datasets/preprocess_imitation_dataset.py \
  --root /media/davidknigge/hard-disk2/storage/robotics/rlbench_20tasks_100episodes \
  --tasks $(find /media/davidknigge/hard-disk2/storage/robotics/rlbench_20tasks_100episodes -mindepth 1 -maxdepth 1 -type d -printf '%f ') \
  --point-cloud-history 2 \
  --proprio-history 2 \
  --future-action-window 16 \
  --future-action-stride 2 \
  --proprio-keys gripper_pose gripper_open \
  --action-keys gripper_pose gripper_open \
  --mask-and-subsample-point-clouds \
  --point-cloud-sample-size 4096
```

The resulting `.rlbench_cache/temporal/<task>/<hash>.h5` files expose precomputed samples with proprio sequences and future actions. The wrapper in `datasets/rlbench_cached.py` converts each cached entry into:

- `observations`: float32 proprio history `(context_length, obs_dim)`
- `actions`: float32 future action rollout `(horizon, action_dim)`

## Training

```bash
mamba activate ppl
python train.py \
    --cache-path /home/dknigge/project_dir/data/robotics/rlbench/imitation_learning \
    --tasks close_drawer open_drawer \
    --output-dir ./artifacts \
    --epochs 50 \
    --batch-size 32 \
    --lr 1e-4 \
    --hidden-dim 512 \
    --num-layers 8 \
    --num-heads 8 \
    --mlp-dim 2048 \
    --use-cuda
```

Key flags:

- `--cache-path`: root directory containing RLBench task folders (defaults to `/home/dknigge/project_dir/data/robotics/rlbench/imitation_learning/`).
- `--tasks`: optional task whitelist; omit to train on every subdirectory detected under `--cache-path`.
- `--output-dir`: directory for checkpoints (defaults to `artifacts/`).
- `--validate-every`: frequency (in epochs) for the sampling-based validation visualization (`1` runs every epoch).
- `--normalization-stats`: path to the JSON generated by `scripts/compute_stats.py` to enable feature normalization.
- Noise schedule is controlled via `--num-train-timesteps`, `--beta-start`, `--beta-end`, and `--beta-schedule`.

Checkpoints are dropped every `--checkpoint-every` epochs (default 5) and always after the final epoch.

## Project Layout

- `src/models/dit/transformer.py` — minimal Diffusion Transformer encoder.
- `src/policies/dit_policy.py` — DiT-backed diffusion policy with simple conditioning.
- `datasets/rlbench_cached.py` — RLBench cache wrapper plus dataloader builder.
- `train.py` — command line entry point for model training.
- `scripts/compute_stats.py` — utility for computing dataset normalization statistics.
- `src/utils/normalization.py` — helpers for applying position/RGB normalizers to batches.
- `.agents_docs/` — operational notes, change log, and job ledger used by the automation agent.

Adapt or extend the modules as needed for your experiments.

## Dataset Normalization

Training can optionally normalize point-cloud positions, RGB values, actions, and proprio features using dataset-wide statistics.

1. **Compute statistics**  
   ```bash
   conda run -n ppl python scripts/compute_stats.py \
       --cache-path /home/dknigge/project_dir/data/robotics/rlbench/imitation_learning \
       --output_dir artifacts/stats \
       --output normalization_stats.json
   ```
   Adjust `--tasks`, `--batch-size`, or `--max-batches` to control which samples contribute.

2. **Enable normalization during training**  
   ```bash
   python train.py \
       --cache-path /home/dknigge/project_dir/data/robotics/rlbench/imitation_learning \
       --normalization-stats artifacts/stats/normalization_stats.json \
       ...
   ```

The JSON file stores means and standard deviations for positions (applied to point clouds, action gripper positions, and proprio history) and for point-cloud RGB values. Validation visuals automatically denormalize so WandB retains correct colors and coordinates.
