"""Lightweight Diffusion Transformer (DiT) encoder."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn


def _modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
    # Affinely transform the normalised activations using conditioning-dependent shift and scale.
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)  # (B, N, D)


@dataclass
class DiffusionTransformerConfig:
    # Hidden size of the token embeddings flowing through the network.
    hidden_dim: int
    # Number of stacked transformer blocks that apply self-attention.
    num_layers: int
    # Count of attention heads per multi-head attention layer.
    num_heads: int
    # Inner dimensionality of the feed-forward network inside each block.
    mlp_dim: int
    # Dropout applied to residual branches and MLPs.
    dropout: float = 0.0
    # Dropout used specifically inside the multi-head attention module.
    attention_dropout: float = 0.0
    # Non-linearity used inside the MLP portion of each transformer block.
    activation: str = "gelu"
    # Whether layer normalisation happens before attention and MLP (True mimics AdaLN-Zero design).
    norm_first: bool = True
    # Numerical stability parameter passed to every LayerNorm.
    layer_norm_eps: float = 1e-5


class _AdaLayerNormZero(nn.Module):
    def __init__(self, hidden_dim: int, eps: float) -> None:
        super().__init__()
        # This LayerNorm removes learnable affine parameters so modulation happens purely via conditioning.
        self.norm = nn.LayerNorm(hidden_dim, elementwise_affine=False, eps=eps)
        self.mlp = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_dim, 2 * hidden_dim, bias=True),
        )
        nn.init.zeros_(self.mlp[-1].weight)
        nn.init.zeros_(self.mlp[-1].bias)

    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:
        # Project the conditioning vector into shift and scale components and modulate the normalised activations.
        shift, scale = self.mlp(cond).chunk(2, dim=-1)  # shift, scale: (B, D)
        return _modulate(self.norm(x), shift, scale)  # (B, N, D)


class DiffusionTransformerBlock(nn.Module):
    def __init__(self, cfg: DiffusionTransformerConfig) -> None:
        super().__init__()
        # Multi-head self-attention mixes the token sequence while respecting conditioning gates downstream.
        self.self_attn = nn.MultiheadAttention(
            embed_dim=cfg.hidden_dim,
            num_heads=cfg.num_heads,
            dropout=cfg.attention_dropout,
            batch_first=True,
        )
        self.attn_norm = nn.LayerNorm(cfg.hidden_dim, elementwise_affine=False, eps=cfg.layer_norm_eps)
        self.attn_dropout = nn.Dropout(cfg.dropout)

        if cfg.activation.lower() == "gelu":
            activation = nn.GELU()
        elif cfg.activation.lower() in {"silu", "swish"}:
            activation = nn.SiLU()
        elif cfg.activation.lower() == "relu":
            activation = nn.ReLU()
        else:
            activation = nn.GELU()

        # Second branch mirrors standard transformer MLPs, again using AdaLN gates for conditioning.
        self.mlp_norm = nn.LayerNorm(cfg.hidden_dim, elementwise_affine=False, eps=cfg.layer_norm_eps)
        self.mlp = nn.Sequential(
            nn.Linear(cfg.hidden_dim, cfg.mlp_dim),
            activation,
            nn.Dropout(cfg.dropout),
            nn.Linear(cfg.mlp_dim, cfg.hidden_dim),
        )
        self.mlp_dropout = nn.Dropout(cfg.dropout)

        # Conditioning network generates per-branch shift/scale and gating scalars.
        self.ada_ln = nn.Sequential(
            nn.SiLU(),
            nn.Linear(cfg.hidden_dim, 6 * cfg.hidden_dim, bias=True),
        )
        nn.init.zeros_(self.ada_ln[-1].weight)
        nn.init.zeros_(self.ada_ln[-1].bias)

    def forward(
        self,
        tokens: torch.Tensor,
        *,
        cond: torch.Tensor,
        attn_mask: Optional[torch.Tensor],
    ) -> torch.Tensor:
        # Split the conditioning signal into the pieces required for attention and MLP residual gates.
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.ada_ln(cond).chunk(6, dim=-1)  # (B, D)

        # Apply AdaLN-Zero modulation before feeding tokens into self-attention.
        attn_in = _modulate(self.attn_norm(tokens), shift_msa, scale_msa)  # (B, N, D)
        attn_out, _ = self.self_attn(attn_in, attn_in, attn_in, attn_mask=attn_mask, need_weights=False)  # attn_out: (B, N, D)
        tokens = tokens + gate_msa.unsqueeze(1) * self.attn_dropout(attn_out)  # (B, N, D)

        # Pass residual through the MLP path with its own modulation and gating.
        mlp_in = _modulate(self.mlp_norm(tokens), shift_mlp, scale_mlp)  # (B, N, D)
        mlp_out = self.mlp_dropout(self.mlp(mlp_in))  # (B, N, D)
        tokens = tokens + gate_mlp.unsqueeze(1) * mlp_out  # (B, N, D)
        return tokens


class DiffusionTransformer(nn.Module):
    def __init__(self, cfg: DiffusionTransformerConfig) -> None:
        super().__init__()
        self.cfg = cfg

        # Instantiate each transformer block in sequence; weight sharing is deliberately avoided.
        self.blocks = nn.ModuleList(DiffusionTransformerBlock(cfg) for _ in range(cfg.num_layers))
        self.final_norm = _AdaLayerNormZero(cfg.hidden_dim, cfg.layer_norm_eps)
        self.apply(self._init_weights)

    @staticmethod
    def _init_weights(module: nn.Module) -> None:
        if isinstance(module, nn.Linear):
            # Xavier keeps activations stable for both attention projections and MLP.
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    @staticmethod
    def _prepare_attention_mask(
        mask: Optional[torch.Tensor],
        *,
        device: torch.device,
        dtype: torch.dtype,
    ) -> Optional[torch.Tensor]:
        if mask is None:
            return None
        if mask.dtype == torch.bool:
            # Convert boolean masks into additive masks understood by PyTorch attention.
            float_mask = mask.to(dtype=dtype)  # (N, N)
            float_mask.masked_fill_(float_mask > 0, float("-inf"))  # (N, N)
            return float_mask

        # Non-boolean masks are assumed to be additive already; just move to the right device/dtype.
        return mask.to(device=device, dtype=dtype)  # (N, N)

    def forward(
        self,
        tokens: torch.Tensor,
        *,
        diffusion_time_cond: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        # Prepare attention mask upfront so each block can reuse the same tensor.
        mask = self._prepare_attention_mask(attn_mask, device=tokens.device, dtype=tokens.dtype)  # (N, N) or None
        x = tokens  # (B, N, D)
        for block in self.blocks:
            # Conditioning signal is broadcast across tokens; each block applies its own AdaLN gates.
            x = block(x, cond=diffusion_time_cond, attn_mask=mask)  # (B, N, D)

        # Final AdaLN zeroes the residual mean before returning the sequence encoding.
        return self.final_norm(x, diffusion_time_cond)  # (B, N, D)


__all__ = ["DiffusionTransformer", "DiffusionTransformerConfig"]
