"""Policy evaluation harness using the RLBench inference environment."""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import h5py
import torch

from rlbench.datasets.imitation_learning.inference_env import (
    ImitationEnvConfig,
    ImitationLearningInferenceEnv,
)

from src.policies import (
    DiTDiffusionPolicy,
    DiTDiffusionPolicyConfig,
    PlatonicDiffusionPolicy,
    PlatonicDiffusionPolicyConfig,
)
from src.utils import NormalizationStats, NormalizationTransform


@dataclass
class EvaluationConfig:
    """Aggregated settings needed to run evaluation."""

    checkpoint_path: Path  # Location of the saved policy checkpoint.
    cache_path: Path  # Cached dataset file used to recover dataset metadata.
    task: str  # RLBench task identifier (e.g. ``reach_target``).
    dataset_root: Optional[Path]  # Optional raw RLBench dataset directory.
    normalization_stats: Optional[Path]  # JSON file with point/action statistics.
    device: str  # Preferred torch device string, e.g. ``cuda`` or ``cpu``.
    policy_override: Optional[str]  # Force loading ``dit`` or ``platonic``.
    episodes: int  # Number of evaluation rollouts to execute.
    max_steps: int  # Upper limit on env steps per episode.
    pointcloud_max_points: Optional[int]  # Optional cap on point cloud width.
    deterministic: bool  # Enable deterministic DDIM sampling where supported.
    seed: Optional[int]  # Global seed for policy sampling and env resets.
    headless: bool  # Whether to launch RLBench without a viewer window.
    image_size: Tuple[int, int]  # (width, height) for rendered camera streams.
    camera_streams: Tuple[str, ...]  # Camera names to mirror from the simulator.
    include_depth: bool  # Mirror depth buffers for selected cameras.
    include_mask: bool  # Mirror segmentation masks for selected cameras.
    log_interval: int  # Frequency for printing per-episode summaries.


@dataclass
class EpisodeResult:
    """Structured statistics collected for each rollout."""

    index: int  # 0-based episode counter.
    reward: float  # Accumulated sparse reward emitted by RLBench.
    steps: int  # Number of simulator steps executed before termination.
    success: bool  # True when the task completed successfully.
    variation: int  # Variation id sampled by the environment.


def parse_args() -> EvaluationConfig:
    """Parse CLI flags and package them inside :class:`EvaluationConfig`."""

    parser = argparse.ArgumentParser(description="Evaluate a trained diffusion policy in RLBench.")
    parser.add_argument("--checkpoint", required=True, type=Path, help="Path to the saved policy checkpoint (.pt).")
    parser.add_argument(
        "--cache-path",
        required=True,
        type=Path,
        help="Temporal cache file generated by rlbench_data_gen (HDF5).",
    )
    parser.add_argument("--task", required=True, type=str, help="RLBench task name to evaluate.")
    parser.add_argument(
        "--dataset-root",
        type=Path,
        default=None,
        help="Raw RLBench dataset directory (enables variation-conditioned rendering).",
    )
    parser.add_argument(
        "--normalization-stats",
        type=Path,
        default=None,
        help="Normalization JSON produced by compute_stats.py (optional).",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="Torch device to run evaluation on (defaults to auto-select CUDA).",
    )
    parser.add_argument(
        "--policy",
        type=str,
        choices=("auto", "dit", "platonic"),
        default="auto",
        help="Override policy family when auto-detection is ambiguous.",
    )
    parser.add_argument("--episodes", type=int, default=5, help="Number of evaluation episodes to run.")
    parser.add_argument(
        "--max-steps",
        type=int,
        default=200,
        help="Hard cap on simulator steps per episode before truncation.",
    )
    parser.add_argument(
        "--pointcloud-max-points",
        type=int,
        default=None,
        help="Optional point-cloud subsampling limit (keep first N points).",
    )
    parser.add_argument(
        "--deterministic",
        action="store_true",
        help="Disable DDIM stochasticity when supported by the policy.",
    )
    parser.add_argument("--seed", type=int, default=None, help="Seed for policy sampling and environment resets.")
    parser.add_argument("--headless", action="store_true", help="Launch RLBench headlessly without a viewer window.")
    parser.add_argument(
        "--image-size",
        type=int,
        nargs=2,
        default=(128, 128),
        metavar=("WIDTH", "HEIGHT"),
        help="Camera resolution forwarded to the RLBench environment.",
    )
    parser.add_argument(
        "--camera-streams",
        type=str,
        nargs="*",
        default=(),
        help="Optional list of camera streams to mirror in observations.",
    )
    parser.add_argument(
        "--include-depth",
        action="store_true",
        help="Request depth images alongside RGB for mirrored cameras.",
    )
    parser.add_argument(
        "--include-mask",
        action="store_true",
        help="Request segmentation masks alongside RGB for mirrored cameras.",
    )
    parser.add_argument(
        "--log-interval",
        type=int,
        default=1,
        help="Episodes between progress log lines (set 0 to silence per-episode logs).",
    )

    args = parser.parse_args()

    device = args.device
    if device == "cuda" and not torch.cuda.is_available():
        device = "cpu"

    policy_override = None if args.policy == "auto" else args.policy

    return EvaluationConfig(
        checkpoint_path=args.checkpoint,
        cache_path=args.cache_path,
        task=args.task,
        dataset_root=args.dataset_root,
        normalization_stats=args.normalization_stats,
        device=device,
        policy_override=policy_override,
        episodes=args.episodes,
        max_steps=args.max_steps,
        pointcloud_max_points=args.pointcloud_max_points,
        deterministic=bool(args.deterministic),
        seed=args.seed,
        headless=bool(args.headless),
        image_size=(int(args.image_size[0]), int(args.image_size[1])),
        camera_streams=tuple(args.camera_streams or ()),
        include_depth=bool(args.include_depth),
        include_mask=bool(args.include_mask),
        log_interval=max(0, int(args.log_interval)),
    )


def _load_cache_config(cache_path: Path) -> Dict[str, object]:
    """Extract dataset metadata stored inside the cached HDF5 file."""

    with h5py.File(cache_path, "r") as handle:
        config_json = handle.attrs.get("config")
        if config_json is None:
            raise ValueError(f"Cache file {cache_path} is missing the 'config' attribute.")
        if isinstance(config_json, bytes):
            config_json = config_json.decode("utf-8")
        return json.loads(config_json)


def _infer_policy_type(config: Dict[str, object], override: Optional[str]) -> str:
    """Decide which policy implementation to instantiate for the checkpoint."""

    if override is not None:
        return override

    if "solid_name" in config or "scalar_channels" in config:
        return "platonic"
    if "point_feature_dim" in config and "proprio_dim" in config:
        return "dit"
    raise ValueError("Unable to infer policy type; please pass --policy explicitly.")


def _instantiate_policy(
    policy_type: str,
    config: Dict[str, object],
    device: torch.device,
) -> torch.nn.Module:
    """Instantiate the requested policy and move it onto ``device``."""

    if policy_type == "dit":
        policy_cfg = DiTDiffusionPolicyConfig(**config)
        policy = DiTDiffusionPolicy(policy_cfg)
    elif policy_type == "platonic":
        policy_cfg = PlatonicDiffusionPolicyConfig(**config)
        policy = PlatonicDiffusionPolicy(policy_cfg)
    else:
        raise ValueError(f"Unsupported policy type '{policy_type}'.")
    return policy.to(device)


def _load_checkpoint(path: Path) -> Dict[str, object]:
    """Load a training checkpoint saved via :func:`torch.save`."""

    checkpoint = torch.load(path, map_location="cpu")
    expected_keys = {"model_state", "config"}
    missing = expected_keys - checkpoint.keys()
    if missing:
        raise KeyError(f"Checkpoint {path} is missing required keys: {sorted(missing)}")
    return checkpoint


def _prepare_policy_inputs(
    observation: Dict[str, object],
    *,
    normalizer: Optional[NormalizationTransform],
    device: torch.device,
    max_points: Optional[int],
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    """Convert a simulator observation into batched tensors for the policy.

    Args:
        observation: Structure returned by :meth:`ImitationLearningInferenceEnv.reset`.
        normalizer: Optional transform mirroring the training-time preprocessing.
        device: Torch device that hosts the policy weights.
        max_points: When provided, truncate each point cloud to this many points.

    Returns:
        proprio: (B=1, N_time, D_proprio) tensor ready for the policy.
        point_cloud: Dict with
            - positions: (B=1, N_time, N_points, 3)
            - colors: (B=1, N_time, N_points, 3)
    """

    obs_block = observation["observation"]
    proprio = obs_block["proprio_sequence"].to(torch.float32)  # (N_time, D)

    point_seq = obs_block["point_cloud_sequence"]
    points = point_seq["points"].to(torch.float32)  # (N_time, N_points, 3)
    colors = point_seq["colors"].to(torch.float32)  # (N_time, N_points, 3)

    masks = point_seq.get("masks")
    if masks is not None:
        masks = masks.to(torch.bool)  # (N_time, N_points)

    if max_points is not None:
        points = points[..., :max_points, :]  # (N_time, N_points_clip, 3)
        colors = colors[..., :max_points, :]  # (N_time, N_points_clip, 3)
        if masks is not None:
            masks = masks[..., :max_points]  # (N_time, N_points_clip)

    if normalizer is not None:
        pc_dict = {"points": points, "colors": colors}
        if masks is not None:
            pc_dict["masks"] = masks
        pc_norm = normalizer.normalize_point_cloud_sequence(pc_dict)
        points = pc_norm["points"]  # (N_time, N_points_clip, 3)
        colors = pc_norm["colors"]  # (N_time, N_points_clip, 3)
        proprio = normalizer.normalize_proprio(proprio)  # (N_time, D)

    proprio = proprio.unsqueeze(0).to(device=device)  # (1, N_time, D)
    positions = points.unsqueeze(0).to(device=device)  # (1, N_time, N_points, 3)
    colours = colors.unsqueeze(0).to(device=device)  # (1, N_time, N_points, 3)

    point_cloud = {
        "positions": positions,
        "colors": colours,
    }
    return proprio, point_cloud


def _denormalize_action(
    actions: torch.Tensor,
    *,
    normalizer: Optional[NormalizationTransform],
) -> torch.Tensor:
    """Invert the optional action normalization for environment execution."""

    if normalizer is None:
        return actions
    return normalizer.denormalize_action_positions(actions)


def _build_env_config(
    cfg: EvaluationConfig,
    cache_cfg: Dict[str, object],
) -> ImitationEnvConfig:
    """Translate CLI + cache metadata into :class:`ImitationEnvConfig`."""

    return ImitationEnvConfig(
        task=cfg.task,
        cache_path=cfg.cache_path,
        dataset_root=str(cfg.dataset_root) if cfg.dataset_root is not None else None,
        image_size=cfg.image_size,
        mask_point_clouds=bool(cache_cfg.get("mask_point_clouds", True)),
        point_cloud_sample_size=int(cache_cfg.get("point_cloud_sample_size", 2048)),
        seed=cfg.seed,
        headless=cfg.headless,
        point_cloud_history=int(cache_cfg.get("point_cloud_history", 2)),
        proprio_history=int(cache_cfg.get("proprio_history", 2)),
        proprio_keys=tuple(cache_cfg.get("proprio_keys", ("gripper_pose", "gripper_open"))),
        camera_streams=cfg.camera_streams,
        camera_include_depth=cfg.include_depth,
        camera_include_mask=cfg.include_mask,
    )


def _check_alignment(policy: torch.nn.Module, cache_cfg: Dict[str, object]) -> None:
    """Ensure policy hyperparameters match the cached dataset geometry."""

    context_length = int(cache_cfg.get("point_cloud_history", 0)) + 1
    horizon = int(cache_cfg.get("future_action_window", 0))

    if hasattr(policy, "cfg"):
        cfg = policy.cfg
        policy_context = getattr(cfg, "context_length", None)
        policy_horizon = getattr(cfg, "horizon", None)
        if policy_context is not None and policy_context != context_length:
            raise ValueError(
                f"Policy context_length {policy_context} != cache context {context_length}."
            )
        if policy_horizon is not None and policy_horizon != horizon:
            raise ValueError(f"Policy horizon {policy_horizon} != cache horizon {horizon}.")


def _episode_summary_log(result: EpisodeResult) -> str:
    """Human-readable one-line summary for a single episode."""

    verdict = "success" if result.success else "failure"
    return (
        f"episode {result.index:02d} | variation {result.variation:02d} | "
        f"reward {result.reward:.2f} | steps {result.steps:03d} | {verdict}"
    )


def _summarize(results: Iterable[EpisodeResult]) -> Dict[str, float]:
    """Aggregate episode statistics into simple scalar metrics."""

    data = list(results)
    if not data:
        return {"success_rate": 0.0, "mean_reward": 0.0, "mean_steps": 0.0}

    successes = sum(1 for item in data if item.success)
    total_reward = sum(item.reward for item in data)
    total_steps = sum(item.steps for item in data)
    count = len(data)
    return {
        "success_rate": successes / count,
        "mean_reward": total_reward / count,
        "mean_steps": total_steps / count,
    }


def evaluate(cfg: EvaluationConfig) -> None:
    """Main evaluation loop coordinating checkpoint, policy, and environment."""

    checkpoint = _load_checkpoint(cfg.checkpoint_path)
    policy_config = checkpoint["config"]
    if not isinstance(policy_config, dict):
        raise TypeError("Checkpoint config must be a dictionary produced by dataclasses.asdict.")

    policy_type = _infer_policy_type(policy_config, cfg.policy_override)
    device = torch.device(cfg.device)
    policy = _instantiate_policy(policy_type, policy_config, device)
    policy.load_state_dict(checkpoint["model_state"])
    policy.eval()

    cache_cfg = _load_cache_config(cfg.cache_path)
    _check_alignment(policy, cache_cfg)

    env_config = _build_env_config(cfg, cache_cfg)
    env = ImitationLearningInferenceEnv(env_config)

    normalizer: Optional[NormalizationTransform] = None
    if cfg.normalization_stats is not None:
        stats = NormalizationStats.from_json(cfg.normalization_stats)
        normalizer = NormalizationTransform(stats)

    generator: Optional[torch.Generator] = None
    if cfg.seed is not None:
        generator = torch.Generator(device=device)
        generator.manual_seed(cfg.seed)

    results: List[EpisodeResult] = []

    try:
        for episode_idx in range(cfg.episodes):
            if cfg.seed is not None:
                env_seed = cfg.seed + episode_idx
            else:
                env_seed = None

            if env_seed is not None:
                torch.manual_seed(env_seed)
            observation = env.reset()
            done = False
            steps = 0
            reward_acc = 0.0
            success = False
            variation = int(observation["meta"].get("variation", -1))

            while not done and steps < cfg.max_steps:
                proprio, point_cloud = _prepare_policy_inputs(
                    observation,
                    normalizer=normalizer,
                    device=device,
                    max_points=cfg.pointcloud_max_points,
                )

                with torch.no_grad():
                    if policy_type == "dit":
                        actions = policy.sample_actions(
                            proprio,
                            point_cloud,
                            generator=generator,
                        )  # (1, horizon, action_dim)
                    else:
                        actions = policy.sample_actions(
                            proprio,
                            point_cloud,
                            generator=generator,
                            deterministic=cfg.deterministic,
                        )  # (1, horizon, 8)

                actions = actions.to(device=device)
                first_action = actions[:, 0, :]  # (1, action_dim)
                first_action = _denormalize_action(
                    first_action,
                    normalizer=normalizer,
                )

                action_cpu = first_action.squeeze(0).to(device="cpu").numpy()  # (action_dim,)
                observation, reward, done, info = env.step(action_cpu)
                reward_acc += reward
                steps += 1
                variation = int(info.get("variation", variation))
                if done and reward > 0.5:
                    success = True

            results.append(
                EpisodeResult(
                    index=episode_idx,
                    reward=reward_acc,
                    steps=steps,
                    success=success,
                    variation=variation,
                )
            )

            if cfg.log_interval > 0 and (episode_idx + 1) % cfg.log_interval == 0:
                print(_episode_summary_log(results[-1]))

    finally:
        env.close()

    summary = _summarize(results)
    print("\nEvaluation complete.")
    print(f"success_rate: {summary['success_rate']:.3f}")
    print(f"mean_reward:  {summary['mean_reward']:.3f}")
    print(f"mean_steps:   {summary['mean_steps']:.1f}")


def main() -> None:
    cfg = parse_args()
    evaluate(cfg)


if __name__ == "__main__":
    main()
